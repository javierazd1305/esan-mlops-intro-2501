{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c335054a-8267-474c-a07e-5a3d8fc383ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1. Preparación: Lectura y procesamiento de la data original\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql.functions import expr, col, min, mean, greatest,max\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2565a59f-bf65-46fc-b117-38637ba65a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos las tablas base\n",
    "base_atributos = spark.table(\"databricks_clase.prueba_schema.base_atributos\")\n",
    "base_cliente = spark.table(\"databricks_clase.prueba_schema.base_cliente\")\n",
    "base_trx = spark.table(\"databricks_clase.prueba_schema.base_trx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c64cd7-5365-4d29-a1ce-7609a62152f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizamos los joins para consolidar la información\n",
    "join_1 = base_atributos.join(base_cliente, on=[\"periodo\", \"id_cliente\"], how=\"left\")\n",
    "tabla_consolidada = join_1.join(base_trx, on=[\"periodo\", \"id_cliente\"], how=\"left\")\n",
    "\n",
    "# Imputación de valores nulos en columnas categóricas\n",
    "tabla_consolidada = tabla_consolidada.fillna({\n",
    "    \"tipo_producto\": \"Desconocido\",\n",
    "    \"departamento\": \"Desconocido\",\n",
    "    \"canal\": \"Desconocido\"\n",
    "})\n",
    "\n",
    "# Imputación en columnas numéricas con la mediana\n",
    "numeric_cols = [\"monto_1m\", \"monto_2m\", \"monto_3m\", \"frecuencia_1m\", \"frecuencia_2m\", \"frecuencia_3m\"]\n",
    "for col_name in numeric_cols:\n",
    "    median_value = tabla_consolidada.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "    tabla_consolidada = tabla_consolidada.fillna({col_name: median_value})\n",
    "\n",
    "# Creación de nuevas características\n",
    "tabla_consolidada = tabla_consolidada.withColumn(\n",
    "    \"monto_total\",\n",
    "    expr(\"monto_1m + monto_2m + monto_3m + monto_4m + monto_5m + monto_6m\")\n",
    ")\n",
    "tabla_consolidada = tabla_consolidada.withColumn(\n",
    "    \"tendencia_monto\",\n",
    "    expr(\"(monto_1m - monto_6m) / monto_6m\")\n",
    ")\n",
    "\n",
    "# Otros ajustes en la data\n",
    "tabla_consolidada = tabla_consolidada.drop('__index_level_0__')\n",
    "tabla_consolidada = tabla_consolidada.filter(col(\"flg_churn\").isNotNull())\n",
    "\n",
    "columnas_cero = [\"incidencias_a\", \"incidencias_b\", \"crossell\", \"ultima_compra_2m\", \"ultima_compra_3m\"]\n",
    "tabla_consolidada = tabla_consolidada.fillna({col: 0 for col in columnas_cero})\n",
    "\n",
    "periodo_minimo = tabla_consolidada.select(min(\"periodo\")).collect()[0][0]\n",
    "tabla_consolidada = tabla_consolidada.fillna({\"periodo_creacion\": periodo_minimo})\n",
    "\n",
    "columnas_menos_uno = [\"segmento_pago\", \"segmento_cliente\"]\n",
    "tabla_consolidada = tabla_consolidada.fillna({col: -1 for col in columnas_menos_uno})\n",
    "\n",
    "tasa_media = tabla_consolidada.select(mean(\"tasa\")).collect()[0][0]\n",
    "tabla_consolidada = tabla_consolidada.fillna({\"tasa\": tasa_media})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8222c05-30c4-4f19-924d-c268ffdc79a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "periodo_max =tabla_consolidada.agg(max(\"periodo\")).collect()[0][0]\n",
    "tabla_consolidada_mensual = tabla_consolidada.filter(tabla_consolidada.periodo == periodo_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a286bbf-3938-496d-bf59-1cf4cbb92f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS databricks_clase.prueba_schema.base_consolidada_v2 PURGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d546ce-d414-42c0-88b9-c0ed31b0004e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE databricks_clase.prueba_schema.base_consolidada_v2 (\n",
    "    periodo BIGINT,\n",
    "    id_cliente BIGINT,\n",
    "    tiempo_permanencia BIGINT,\n",
    "    flg_vip DOUBLE,\n",
    "    incidencias_a DOUBLE NOT NULL,\n",
    "    incidencias_b DOUBLE NOT NULL,\n",
    "    tipo_producto STRING NOT NULL,\n",
    "    periodo_creacion BIGINT NOT NULL,\n",
    "    departamento STRING NOT NULL,\n",
    "    segmento_pago BIGINT NOT NULL,\n",
    "    canal STRING NOT NULL,\n",
    "    segmento_cliente BIGINT NOT NULL,\n",
    "    crossell DOUBLE NOT NULL,\n",
    "    tasa DOUBLE NOT NULL,\n",
    "    monto_1m DOUBLE NOT NULL,\n",
    "    monto_2m DOUBLE NOT NULL,\n",
    "    monto_3m DOUBLE NOT NULL,\n",
    "    monto_4m DOUBLE,\n",
    "    monto_5m DOUBLE,\n",
    "    monto_6m DOUBLE,\n",
    "    cantidad_1m DOUBLE,\n",
    "    cantidad_2m DOUBLE,\n",
    "    cantidad_3m DOUBLE,\n",
    "    cantidad_6m DOUBLE,\n",
    "    frecuencia_1m DOUBLE NOT NULL,\n",
    "    frecuencia_2m DOUBLE NOT NULL,\n",
    "    frecuencia_3m DOUBLE NOT NULL,\n",
    "    ultima_compra_1m DOUBLE,\n",
    "    ultima_compra_2m DOUBLE NOT NULL,\n",
    "    ultima_compra_3m DOUBLE NOT NULL,\n",
    "    flg_churn DOUBLE,\n",
    "    monto_total DOUBLE,\n",
    "    tendencia_monto DOUBLE\n",
    ")\n",
    "USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4dd6e58-00b4-44b4-b52b-6dfe0f51e737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos la tabla consolidada en Unity Catalog para persistencia\n",
    "tabla_consolidada.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"databricks_clase.prueba_schema.base_consolidada_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00dadeb-3dba-4804-9257-bd26a9fd4be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. Creación de dos Feature Store tables\n",
    "# -----------------------------------------------------------\n",
    "fs = FeatureStoreClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51be4785-5cd4-464f-b17e-6ec1e55d4b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/26 02:11:31 INFO databricks.ml_features._compute_client._compute_client: Setting columns ['id_cliente'] of table 'databricks_clase.prueba_schema.base_consolidada_mensual_feats_v2' to NOT NULL.\n2025/02/26 02:11:32 INFO databricks.ml_features._compute_client._compute_client: Setting Primary Keys constraint ['id_cliente'] on table 'databricks_clase.prueba_schema.base_consolidada_mensual_feats_v2'.\n/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:60: SyntaxWarning: invalid escape sequence '\\.'\n  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:2570: SyntaxWarning: invalid escape sequence '\\.'\n  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:3431: SyntaxWarning: invalid escape sequence '\\.'\n  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n2025/02/26 02:11:35 INFO databricks.ml_features._compute_client._compute_client: Created feature table 'databricks_clase.prueba_schema.base_consolidada_mensual_feats_v2'.\n"
     ]
    }
   ],
   "source": [
    "# --- 2.1. Tabla 1: Features mensuales (conjunto principal de features) ---\n",
    "feature_columns_1 = [\n",
    "    'periodo', 'id_cliente', 'tiempo_permanencia', 'flg_vip', 'incidencias_a', 'incidencias_b', \n",
    "    'tipo_producto', 'periodo_creacion', 'departamento', 'segmento_pago', 'canal', \n",
    "    'segmento_cliente', 'crossell', 'tasa', 'monto_1m', 'monto_2m', 'monto_3m', 'monto_4m', \n",
    "    'monto_5m', 'monto_6m', 'cantidad_1m', 'cantidad_2m', 'cantidad_3m', 'cantidad_6m', \n",
    "    'frecuencia_1m', 'frecuencia_2m', 'frecuencia_3m', 'ultima_compra_1m', 'ultima_compra_2m', \n",
    "    'ultima_compra_3m', 'monto_total', 'tendencia_monto'\n",
    "]\n",
    "\n",
    "# Suponemos que ya se ha generado la data mensual consolidada\n",
    "df_mensual = spark.table(\"databricks_clase.prueba_schema.base_consolidada_v2\")\n",
    "\n",
    "feature_table_name_1 = \"databricks_clase.prueba_schema.base_consolidada_mensual_feats_v2\"\n",
    "\n",
    "# Crear la tabla de features 1 en el Feature Store\n",
    "fs.create_table(\n",
    "    name=feature_table_name_1,\n",
    "    primary_keys=[\"id_cliente\"],\n",
    "    schema=df_mensual.select(*feature_columns_1).schema,\n",
    "    description=\"Feature Store con features mensuales consolidados (sin flg_churn)\"\n",
    ")\n",
    "\n",
    "# Escribir la data en el Feature Store (excluimos 'id_cliente' de las columnas de features, pues es clave primaria)\n",
    "columns_to_write_1 = [col for col in feature_columns_1 if col != \"id_cliente\"]\n",
    "df_features = df_mensual.select(\"id_cliente\", *columns_to_write_1).dropDuplicates([\"id_cliente\"])\n",
    "fs.write_table(\n",
    "    name=feature_table_name_1,\n",
    "    df=df_features,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1b8c6b-6dc6-49b4-a805-b4d40ac76795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2.2. Tabla 2: Features históricos (simulando otro set de características derivadas) ---\n",
    "# En este ejemplo, calculamos dos features adicionales a partir de la data mensual\n",
    "df_historico = df_mensual.withColumn(\"avg_monto\", expr(\"(monto_1m + monto_2m + monto_3m) / 3\")) \\\n",
    "                         .withColumn(\"max_monto\", greatest(col(\"monto_1m\"), col(\"monto_2m\"), col(\"monto_3m\")))\n",
    "\n",
    "feature_columns_2 = [\"id_cliente\", \"avg_monto\", \"max_monto\"]\n",
    "feature_table_name_2 = \"databricks_clase.prueba_schema.historico_feats\"\n",
    "df_features_2 = df_historico.select(*feature_columns_2).dropDuplicates([\"id_cliente\"])\n",
    "\n",
    "# Crear la tabla de features 2 en el Feature Store\n",
    "fs.create_table(\n",
    "    name=feature_table_name_2,\n",
    "    primary_keys=[\"id_cliente\"],\n",
    "    schema=df_historico.select(*feature_columns_2).schema,\n",
    "    description=\"Feature Store con features históricos derivados (promedio y máximo de monto)\"\n",
    ")\n",
    "\n",
    "# Escribir la data en la tabla de features históricos\n",
    "\n",
    "fs.write_table(\n",
    "    name=feature_table_name_2,\n",
    "    df=df_features_2,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a31d1a-48a3-4137-a6a8-0a9f4b01c236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3. Unir las dos tablas de Feature Store y obtener la data para entrenamiento/inferencia\n",
    "# -----------------------------------------------------------\n",
    "# Lectura de las dos tablas de Feature Store\n",
    "features_df1 = fs.read_table(name=feature_table_name_1)\n",
    "features_df2 = fs.read_table(name=feature_table_name_2)\n",
    "\n",
    "# Realizamos el join usando la clave primaria 'id_cliente'\n",
    "features_joined = features_df1.join(features_df2, on=\"id_cliente\", how=\"inner\")\n",
    "\n",
    "# Para obtener el target, unimos la data de features con la tabla consolidada original\n",
    "target_df = spark.table(\"databricks_clase.prueba_schema.base_consolidada_v2\").select(\"id_cliente\", \"flg_churn\")\n",
    "features_final = features_joined.join(target_df, on=\"id_cliente\", how=\"inner\")\n",
    "\n",
    "# Convertimos a Pandas para usar con scikit-learn\n",
    "features_pd = features_final.toPandas()\n",
    "\n",
    "# Definimos las features (columnas) y el target para el modelo\n",
    "feature_list = [\n",
    "    'frecuencia_1m', 'cantidad_1m', 'tasa', 'ultima_compra_1m', 'monto_total', \n",
    "    'avg_monto', 'max_monto'\n",
    "]\n",
    "X = features_pd[feature_list]\n",
    "y = features_pd[\"flg_churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c44d668-780c-4d8a-aac9-88a7462d7093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (entrenamiento): 0.4418\nAccuracy (entrenamiento): 0.7717\nReporte de clasificación (entrenamiento):\n               precision    recall  f1-score   support\n\n         0.0       0.96      0.78      0.86     68203\n         1.0       0.31      0.74      0.44      9491\n\n    accuracy                           0.77     77694\n   macro avg       0.64      0.76      0.65     77694\nweighted avg       0.88      0.77      0.81     77694\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e233e2cbd30043efb01faa26d8dab450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346f2ab57ba34452a0b133d92dae2595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/26 02:14:33 INFO mlflow.tracking._tracking_service.client: 🏃 View run dapper-wolf-739 at: adb-106485471189205.5.azuredatabricks.net/ml/experiments/d6aca2d1639b4b1498475591096c2c2c/runs/8be5abd7b4254acbb00041830e627f47.\n2025/02/26 02:14:33 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: adb-106485471189205.5.azuredatabricks.net/ml/experiments/d6aca2d1639b4b1498475591096c2c2c.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4. Entrenamiento simple del modelo\n",
    "# -----------------------------------------------------------\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Dividimos la data en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Creamos y entrenamos un modelo simple (Random Forest)\n",
    "    model_train = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight=\"balanced\")\n",
    "    model_train.fit(X_train, y_train)\n",
    "    \n",
    "    # Realizamos predicciones en el conjunto de prueba\n",
    "    y_pred_train = model_train.predict(X_test)\n",
    "    \n",
    "    signature = infer_signature(X_test, y_test)  # Inferimos la firma con datos de prueba y predicciones\n",
    "\n",
    "    # Calculamos métricas de desempeño\n",
    "    f1 = f1_score(y_test, y_pred_train)\n",
    "    accuracy = accuracy_score(y_test, y_pred_train)\n",
    "    print(f\"F1 Score (entrenamiento): {f1:.4f}\")\n",
    "    print(f\"Accuracy (entrenamiento): {accuracy:.4f}\")\n",
    "    print(\"Reporte de clasificación (entrenamiento):\\n\", classification_report(y_test, y_pred_train))\n",
    "    \n",
    "    # Registramos parámetros y el modelo en MLflow\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    #mlflow.sklearn.log_model(model_train, \"modelo_simple\")\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model_train,\n",
    "        artifact_path=\"modelo_simple\",\n",
    "        signature=signature,  # Agregamos la firma aquí\n",
    "        input_example=X_test.iloc[:5]  # Opcional pero recomendable para referencia\n",
    "    )\n",
    "    # Opcional: registrar el modelo en el Model Registry\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b116899-f278-4b08-a418-cd3a1e5eabe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8916190051844032,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06 ModelOps - Model Registry",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}